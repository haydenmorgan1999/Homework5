---
title: "Homework 5"
author: "Hayden Morgan"
format: pdf
editor_options: 
  chunk_output_type: console
---

## Task 1: Conceptual Questions

* **Question 1: What is the purpose of using cross-validation when fitting a random forest model?**
  
  + Random forest models include hyperparameters such as the number of predictors that are sampled at each split. Through the process of tuning, hyperparameter values for random forest models can be estimated and selected for the best overall combination in order to produce the best model (e.g. most accurate model, model that is best at generalizing to unseen data). Cross-validation is an important part of the tuning process because it allows for different combinations of hyperparameters to be applied to different subsets of training data before ultimately sending the best model to the test data. Therefore, cross-validation can help avoid overfitting in the process of tuning random forest models.
  
* **Question 2: Describe the bagged tree algorithm.**
  
  + 'Bagging' refers to bootstrap aggregation. 'Bootstrapping' refers to resampling (either from data or a fitted model). In bootstrapping, multiple resamples are taken from an original data sample. Then, for the bagged tree algorithm, tree models are fit to each resample. From there, a prediction can be generated for each tree. At the end, the predictions are combined in some meaningful way to give a final prediction. For example, for a regression tree you can combine predictions through averaging. For classification trees, you could use majority rule to identify the most common prediction from all trees from the bootstrapping step.
  
* **Question 3: What is meant by a general linear model?**

  + General linear model (GLM) refers to a group of models that can all use a linear equation to express a continuous outcome. For example, both simple and multiple linear regression models are GLMs, as well as ANOVAs. However, for GLMs, normal distribution is assumed, while generalized linear models can handle additional models such as logistic regression because they don't depend on normal distribution specifically. 
  
* **Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?**

  + Adding an interaction effect means that you are investigating if the effect of one variable depends on the value of a different variable. In an expression, the interaction term would include two predictor variables multiplied together with the beta term. When the interaction term is NOT included, you can still investigate things like main effects (beta term multiplied by a predictor variable) e.g. if you want to discuss the effects of different predictors--but you would only be able to see if the predictors have main effects, not if they depend on each other. Including either/both main effects and interaction terms can make the model more flexible overall.
  
* **Question 5: Why do we split our data into a training and test set?**

  + We split data into training and test sets because we want a model that is generalizeable and applicable to data it has yet to see. If you do not reserve part of your data as test data and thus train your model on the entirety of your data set, you have no way of knowing if your model only works really well with the data you've already given it, and not as well with similar data that it hasn't seen. This can be problematic assuming that you are creating a model to work with real-world data: you want to create something that can be applied to future data that is unknown in order to continue making valid predictions within the context of the model. If you use training data for your model and it works really well, but when run on test data your model doesn't perform as well, that's a cue to re-tune parameters/hyperparameters and try to get more accurate output. 

## Task 2: Data Prep

### packages and data 
```{r, message=FALSE, warning=FALSE}
library("tidyverse")
library("tidymodels")
library("caret")
library("yardstick")

data <- read.csv("data/heart.csv")

data <- as.tibble(data)

data #proof of data as tibble 
```

### Question 1
```{r}

summary(data)

```

**What type of variable (in R) is Heart Disease? Categorical or Quantitative?**

According to the tibble created in the previous step, Heart Disease is an integer in R, meaning it is quantitative. 

**Does this make sense? Why or why not.**
No, this does not make sense. It is true that Heart Disease includes numbers like in quantitative data, but the numbers are just 0 and 1 and evidenced here:
```{r}
print(data$HeartDisease)
```
This indicates that binary form is being used to indicate that Heart Disease is categorical rather than quantitative. This is further confirmed by visiting the website provided in the homework ([here](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)) where it is stated that a 1 means heart disease is present and 0 means the patient is normal (no heart disease). 

Because of this binary data, treating Heart Disease like integer data and performing quantitative data analysis will not give us the insights desired for the data set. Rather, Heart Disease should be treated as categorical data in order for us to complete predictions and inferences on the data set that make sense in the context of the data. 

### Question 2
```{r}

new_heart <- data |> 
  mutate(HeartDisease = factor(HeartDisease)) |>
  rename(HeartDisease_factor = HeartDisease) |>
  select(-ST_Slope)

colnames(new_heart) #showing that HeartDisease has been replaced and there is no more ST_Slope

```

## Task 3: EDA

### Question 1
```{r}

ggplot(new_heart, aes(x = MaxHR, y = Age, color = HeartDisease_factor))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  labs(x = "Maximum Heart Rate (bpm)", y = "Age (Years)", title = "Age as a Function of Heart
Disease and Max HR")+
  scale_color_viridis_d(name = "Has Heart Disease", labels = c("False", "True"))
  
```

### Question 2

**Based on visual evidence, do you think an interaction model or an additive model is
more appropriate? Justify your answer.**

Based on visual evidence, I think an interaction model may be more appropriate for this data than an additive model. An interaction model assumes that the effect of one predictor (e.g. maximum HR) depends on the level of a different predictor (e..g having heart disease). In this graph, it is clear that the slope between age and maximum HR for those without heart disease is much more negative than the slope between age and maximum HR for those WITH heart disease, so this indicates that there may be other factors (like max HR and heart disease interacting with each other) at play than just maximum HR and age. If an additive model was indicated instead, I would expect there to be less difference in the slopes of the two heart disease groups.

## Task 4: Testing and Training 

## Task 5: OLS and LASSO

## Task 6: Logistic Regression