---
title: "Homework 5"
author: "Hayden Morgan"
format: pdf
editor_options: 
  chunk_output_type: console
---

## Task 1: Conceptual Questions

Tyoe each question as part of a bulleted list and answer each question underneath each bullet.

* **Question 1: What is the purpose of using cross-validation when fitting a random forest model?**
  
  + Random forest models include hyperparameters such as the number of predictors that are sampled at each split. Through the process of tuning, hyperparameter values for random forest models can be estimated and selected for the best overall combination in order to produce the best model (e.g. most accurate model, model that is best at generalizing to unseen data). Cross-validation is an important part of the tuning process because it allows for different combinations of hyperparameters to be applied to different subsets of training data before ultimately sending the best model to the test data. Therefore, cross-validation can help avoid overfitting in the process of tuning random forest models.
  
* **Question 2: Describe the bagged tree algorithm.**
  
  + 'Bagging' refers to bootstrap aggregation. 'Bootstrapping' refers to resampling (either from data or a fitted model). In bootstrapping, multiple resamples are taken from an original data sample. Then, for the bagged tree algorithm, tree models are fit to each resample. From there, a prediction can be generated for each tree. At the end, the predictions are combined in some meaningful way to give a final prediction. For example, for continuous numerical responses you can combine predictions through averaging. For classification trees, you could use majority rule to identify the most common prediction from all trees from the bootstrapping step.
  
* **Question 3: What is meant by a general linear model?**

  + General linear model (GLM) refers to a group of models that can all use a linear equation to express a continuous outcome. For example, both simple and multiple linear regression models are GLMs, as well as ANOVAs. However, for GLMs, normal distribution is assumed, while generalized linear models can handle additional models such as logistic regression because they don't depend on normal distribution specifically. 
  
* **Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?**

  + Adding an interaction effect means that you are investigating if the effect of one variable depends on the value/level of a different variable. In an expression, the interaction term would include two predictor variables multiplied together with the beta term. When the interaction term is NOT included, you can still investigate things like main effects (beta term multiplied by a predictor variable) e.g. if you want to discuss the effects of different predictors--but you would only be able to see if the predictors have main effects, not if they depend on each other. Including either/both main effects and interaction terms can make the model more flexible/complex overall. There is a risk of overfitting the model, though, if an interaction term is included when it is not needed.
  
* **Question 5: Why do we split our data into a training and test set?**

  + We split data into training and test sets because we want a model that is generalizeable and applicable to data it has yet to see. If you do not reserve part of your data as test data and thus train your model on the entirety of your data set, you have no way of knowing if your model only works really well with the data you've already given it, and not as well with similar data that it hasn't seen. This can be problematic assuming that you are creating a model to work with real-world data: you want to create something that can be applied to future data that is unknown in order to continue making valid predictions within the context of the model. If you use training data for your model and it works really well, but when run on test data your model doesn't perform as well, that's a cue to re-tune parameters/hyperparameters and try to get more accurate output. 

## Task 2: Data Prep

### packages and data 

First, create a sub-header titled packages and data and library the tidyverse, tidymodels, caret, and the yardstick package. We will need these our homework. In the code chunk setting, suppress all messages associated with calling these packages.

In the same code chunk, read in your data set as a tibble.
```{r, message=FALSE, warning=FALSE}
#suppressed messages/warnings in code chunk
library("tidyverse")
library("tidymodels")
library("caret")
library("yardstick")

data <- read.csv("data/heart.csv")

data <- as.tibble(data)

data #proof of data as tibble 
```

### Question 1

Run and report summary() on your data set.
```{r}

summary(data)

```

**What type of variable (in R) is Heart Disease? Categorical or Quantitative?**

According to the tibble created in the previous step, Heart Disease is an integer in R, meaning it is quantitative. 

**Does this make sense? Why or why not.**

No, this does not make sense. It is true that Heart Disease includes numbers like in quantitative data, but the numbers are just 0 and 1 and evidenced here:
```{r}

print(data$HeartDisease)

```
This indicates that binary form is being used to indicate that Heart Disease is categorical rather than quantitative. This is further confirmed by visiting the website provided in the homework ([here](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)) where it is stated that a 1 means heart disease is present and 0 means the patient is normal (no heart disease). 

Because of this binary data, treating Heart Disease like integer data and performing quantitative data analysis will not give us the insights desired for the data set. Rather, Heart Disease should be treated as categorical data in order for us to complete predictions and inferences on the data set that make sense in the context of the data. 

### Question 2
```{r}

new_heart <- data |> #new data set is new_heart 
  mutate(HeartDisease = factor(HeartDisease)) |> #changed data type, 
  #removed old HeartDisease
  rename(HeartDisease_factor = HeartDisease) |> #renamed HeartDisease
  select(-ST_Slope) #remove ST_Slope

colnames(new_heart) #showing that HeartDisease has been replaced 
#and there is no more ST_Slope

```

## Task 3: EDA

### Question 1

We are going to model someoneâ€™s age (our response variable) as a function of heart disease and their max heart rate. First, create the appropriate scatterplot to visualize this relationship.
```{r}

ggplot(new_heart, aes(x = MaxHR, y = Age, color = HeartDisease_factor))+ 
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+ #add a line and remove SE bars
  labs(x = "Maximum Heart Rate (bpm)", y = "Age (Years)", #add labs 
  title = "Age as a Function of Heart Disease and Max HR")+
  scale_color_viridis_d(name = "Has Heart Disease", labels = c("False", "True")) 
  #colorblind friendly palette
  
```

### Question 2

**Based on visual evidence, do you think an interaction model or an additive model is more appropriate? Justify your answer.**

Based on visual evidence, I think an interaction model may be more appropriate for this data than an additive model. An interaction model assumes that the effect of one predictor (e.g. maximum HR) depends on the level of a different predictor (e..g having heart disease). In this graph, it is clear that the slope between age and maximum HR for those without heart disease is much more negative than the slope between age and maximum HR for those WITH heart disease, so this indicates that there may be other factors (like max HR and heart disease interacting with each other) at play than just maximum HR and age. If an additive model was indicated instead, I would expect there to be less difference in the slopes of the two heart disease groups.

## Task 4: Testing and Training 

Split your data into a training and test set.
```{r}

set.seed(101) #seed to 101

new_heart_split <- initial_split(new_heart, prop = 0.8) #80/20 split
train <- training(new_heart_split)
test <- testing(new_heart_split)

train
test

```

## Task 5: OLS and LASSO

### Question 1

First fit an interaction model (named ols_mlr) with age as your response, and max heart rate + heart disease as your explanatory variables using the training data set using ordinary least squares regression.
```{r}

ols_mlr <- lm(Age ~ MaxHR + HeartDisease_factor + MaxHR * HeartDisease_factor, 
              data = train)

summary(ols_mlr) #report summary output

```

### Question 2
```{r}

tested <- predict(ols_mlr, newdata = test) #test on testing data set 

rmse_vec(test$Age, tested) #this is the RMSE value I want to report

```

### Question 3
```{r}

folds <- vfold_cv(train, 10) #CV, 10 fold

LASSO_recipe <- recipe(Age ~ MaxHR + HeartDisease_factor, data  = train) |>
  step_normalize(all_numeric_predictors()) |> #standardize predictors
  step_dummy(HeartDisease_factor) |>
  step_interact(~ MaxHR:starts_with("HeartDisease_factor_")) 
  
LASSO_recipe #print recipe 

```

### Question 4
```{r}

spec <- linear_reg(penalty = tune(), mixture = 1) |> #set appropriate spec
  set_engine("glmnet")

wfl <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(spec)

wfl

grid <- wfl |> #set appropriate grid
  tune_grid(resamples = folds,
            grid = grid_regular(penalty(), levels = 10))

final_model <- grid |>
  select_best(metric = "rmse") 

final_wfl <- wfl |> #select final model
  finalize_workflow(final_model) |>
  fit(train)

tidy(final_wfl) #report with tidy

```

### Question 5

**Without looking at the RMSE calculations, would you expect the RMSE calculations to be roughly the same or different? Justify your answer using output from your LASSO model.**

Judging by the LASSO model output, I'd expect the RMSE calculations to be roughly the same, given the extremely small penalties for the LASSO model. Because the penalties are so small, LASSO likely did not shrink the model coefficients much. So, RMSE is probably similar across the models.

### Question 6

Now compare the RMSE between your OLS and LASSO model and show that the RMSE
calculations were roughly the same.
```{r}

tested <- predict(ols_mlr, newdata = test)

OLS <- rmse_vec(test$Age, tested) 

LASSO <- final_wfl |>
  predict(test) |> #eval on test data 
  pull() |>
  rmse_vec(truth = test$Age)

OLS
LASSO

#the RMSEs are are roughly the same, indeed 

```

### Question 7

**Why are the RMSE calculations roughly the same if the coefficients for each model are different?**

The LASSO model can shrink coefficients in a model, e.g. if some predictors are irrelevant, etc. Given the small LASSO penalties and the fact that coefficients in LASSO were not near 0, both the LASSO and OLS models likely contain predictors that are actually useful to the context of the data set and data analysis. Therefore, because there was little difference in the structure of the models even after LASSO, this could explain why the RMSE calculations are roughly the same: the LASSO model did not find it necessary to drastically change the anatomy of the model compared to OLS so their outcomes are very similar. 

## Task 6: Logistic Regression

### Question 1

Propose two different logistic regression models with heart disease as our response.

Fit those models on the training set, using repeated CV.
```{r}

set.seed(101)

split_log <- initial_split(new_heart, prop = 0.8)
train_log <- training(split_log)
test_log <- testing(split_log)
folds_log <- vfold_cv(train_log, 10)

log1_rec <- recipe(HeartDisease_factor ~ Age, data = train_log) |>
  step_normalize(all_numeric()) 
log2_rec <- recipe(HeartDisease_factor ~ Age + MaxHR, data = train_log) |>
  step_normalize(all_numeric()) 

spec_log <- logistic_reg() |>
  set_engine("glm")

log1_wfl <- workflow() |>
  add_recipe(log1_rec) |>
  add_model(spec_log)
log2_wfl <- workflow() |>
  add_recipe(log2_rec) |>
  add_model(spec_log)

log1_fit <- log1_wfl |>
  fit_resamples(folds_log, metrics = metric_set(accuracy, mn_log_loss))
log2_fit <- log2_wfl |>
  fit_resamples(folds_log, metrics = metric_set(accuracy, mn_log_loss))

rbind(log1_fit |> collect_metrics(),
      log2_fit |> collect_metrics()) |>
  mutate(Model = c("Model1", "Model1", "Model2", "Model2")) |>
  select(Model, everything())

```

**Identify your best performing model. Justify why this is your best performing model and provide a basic summary of it.**

The best performing model is Model 2. According to the tibble produced, Model 2 has both a larger accuracy and a smaller log loss than Model 1, both of which are favorable in this context. Model 2 is a more complex model than Model 1, including both Age and MaxHR whereas Model 1 just includes Age. In this case, it looks like adding more predictors to explain data trends makes the model better. This is not always true in data analysis scenarios, sometimes a simpler model ends up fitting data better- but not in this case. 

### Question 2

Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.
```{r}

log2_train_fit <- log2_wfl |>
  fit(train_log) 

conf_mat(train_log |>
           mutate(estimate = log2_train_fit |>
                    predict(train_log) |>
                    pull()),
         HeartDisease_factor,
         estimate)

```

### Question 3

**Next, identify the values of sensitivity and specificity, and interpret them in the context of the problem.**

Sensitivity is a true positive rate, or the probability of a positive test result that is truly positive. Based on the results of the confusion matrix, the sensitivity of Model 2 is as follows:

number of true positives / (number of true positives + number of false negatives) = 

325 / (325 + 93) = 0.7775

In the context of this data set/problem, this means that the logistic regression model Model 2 has a 77.75% chance of positively identifying that heart disease is present when the patient does actually have heart disease. 

Specificity is a true negative rate, or the probability of a negative test result that is truly negative. Based on the results of the confusion matrix, the specificity of Model 2 is as follows:

number of true negatives / (number of true negatives + number of false positives) = 

178 / (178 + 138) = 0.5633

In the context of this data set/problem, this means that the logistic regression model Model 2 has a 56.33% chance of NOT finding heart disease when the patient, in fact, does not have heart disease.

So, overall, the model is pretty good at identifying if a patient has heart disease when they do, but not as good at identifying if the patient does NOT have heart disease when they don't. The model is more likely to return false negatives than false positives. 
